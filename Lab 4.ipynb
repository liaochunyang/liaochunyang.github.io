{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 (Feb 9): Kernel Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will apply kernel method to support vector machine, which allows us to generate non-linear separation boundary line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set $\\{x_i,y_i\\}$ cannot be linearly separated in a lower-dimensional vector space $\\mathbb{R}^n$, while it can be linearly separated in a higher-dimensional vector space $\\mathbb{R}^N$, $n\\leq N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we first introduce feature map $\\phi:\\mathbb{R}^n \\to \\mathbb{R}^N$ that maps $x_i$ to $\\phi(x_i)\\in\\mathbb{R}^N$. Then we apply support vector machine for new data set $\\{\\phi(x_i),y_i\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation in $\\mathbb{R}^N$ is the main difficulty. It's addressed by introducing kernel function that gives us a simple way to compute the inner product in $\\mathbb{R}^N$.\n",
    "$$ K(x_i,x_j) = \\langle \\phi(x_i),\\phi(x_j) \\rangle $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall support vector machine, we try to find $w\\in\\mathbb{R}^N$ and $b\\in\\mathbb{R}$. According to the textbook, $w = \\sum_{j=1}^{m}a_j\\phi(x_j)$ for some $a\\in\\mathbb{R}^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The desired $a\\in\\mathbb{R}^m$ and $b\\in\\mathbb{R}$ are the solution to the following optimization program\n",
    "$$ \\mathop{\\mathrm{minimize}}_{a\\in\\mathbb{R}^m, b\\in\\mathbb{R},\\xi\\in\\mathbb{R}^m} a^\\top Ka + \\frac{1}{\\lambda}\\sum_{i=1}^{m}\\xi_i \\quad \\mbox{ s.to } \\quad y_i((Ka)_i-b)\\geq1-\\xi_i \\quad  \\mbox{ for all } i\\in[1 : m],$$\n",
    "where $K_{i,j} = K(x_i,x_j) = \\langle \\phi(x_i),\\phi(x_j) \\rangle$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function implements above optimization problem using CVXPY. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_svm(K,lamb,m0,m1):\n",
    "    \n",
    "    ## K is the kernel matrix.\n",
    "    ## Hyperparameter lamb needs to be chosen carefully.\n",
    "    ## m0 is the number of data points with label y = -1\n",
    "    ## m1 is the number of data points with label y = +1\n",
    "    \n",
    "    # define the variables a\\in R^m, b\\in R and xi\\in R^m (m = m0+m1)\n",
    "    \n",
    "    \n",
    "    # define the objective function\n",
    "    # (Hint: compute the quadratic form in CVXPY(cp): cp.quad_form(a,K)  a is vector, K is matrix )\n",
    "    \n",
    "    \n",
    "    # define the inequality constraints\n",
    "    # First m0 costraints correspond to class with label -1\n",
    "    # Next m1 constraints correspond to class with label +1\n",
    "    \n",
    "    \n",
    "    # solve optimization problem\n",
    "    \n",
    "    \n",
    "    # return the outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaination of polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "m0 = 100;\n",
    "m1 = 50;\n",
    "\n",
    "# The first coordinate of X0\n",
    "X0_x = np.random.uniform(0,6,m0)\n",
    "# The second coordinate of X0\n",
    "X0_y = np.sin(X0_x) + np.random.uniform(0,0.5,m0) \n",
    "# Stack first and second coordinate to obatin X0\n",
    "X0 = np.column_stack( (X0_x, X0_y) )\n",
    "    \n",
    "# The first coordinate of X1\n",
    "X1_x = np.random.uniform(0,6,m1) \n",
    "# The second coordinate of X1\n",
    "X1_y = np.sin(X1_x) - np.random.uniform(0,0.5,m1)\n",
    "# Stack first and second coordinate to obtain X1\n",
    "X1 = np.column_stack( (X1_x, X1_y) )\n",
    "\n",
    "# Given data set X0,X1, we stack them as a big data matrix X = [X0;X1]\n",
    "X = np.row_stack((X0,X1)) \n",
    "\n",
    "# Visualize data\n",
    "plt.plot(X0[:,0],X0[:,1],'b.',X1[:,0],X1[:,1],'r+', markerfacecolor='none',markersize=8)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function computes the kernel matrix corresponding to polynomial kernel of degree d, which is computed by\n",
    "$$ K(x,y) = (1+\\langle x,y\\rangle)^d, \\quad \\mbox{ for all } x,y\\in\\mathbb{R}^n.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_kernel(X,d):\n",
    "       \n",
    "    # Compute kernel matrix K (Hint: matrix transpose helps a lot, using loop is slow)\n",
    "    \n",
    "    \n",
    "    # return kernel matrix K\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the kernel matrix K_poly\n",
    "d = 3\n",
    "K_poly = poly_kernel(X,d)\n",
    "\n",
    "# Compute the optimization problem to get a\\in R^m and b\\in R\n",
    "lamb = 1e-3\n",
    "a_poly,b_poly = kernel_svm(K_poly,lamb,m0,m1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the boundary produced by SVM with the polynomial kernel\n",
    "plt.plot(X0[:,0],X0[:,1],'b.',X1[:,0],X1[:,1],'r+',markerfacecolor='none',markersize=8)\n",
    "grid_x = np.linspace(0,6,100)\n",
    "grid_y = np.linspace(-1.5,1.5,100)\n",
    "xx,yy = np.meshgrid(grid_x,grid_y)\n",
    "f = sum( ( a_poly[i] *(1+X[i,0]*xx+X[i,1]*yy)**3 for i in range(m0+m1) ) )  \n",
    "plt.contour(grid_x,grid_y,f-b_poly,0, colors='k',linestyles='dashed')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaination of Gaussian Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function computes the kernel matrix of Gaussian kernel, which is computed by \n",
    "$$ K(x,y) = \\exp(-\\frac{\\|x-y\\|^2}{2\\sigma^2}), \\quad \\mbox{ for all } x,y\\in\\mathbb{R}^n.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(X,sigma):\n",
    "    \n",
    "    # X is the data matrix; sigma is the parameter in Gaussian kernel\n",
    "    \n",
    "    # use np.shape(X)[0] to get the number of data points\n",
    "    \n",
    "    \n",
    "    # Compute kernel matrix K.\n",
    "    # (Hint: One cannot avoid two layers for loop. The range of for loop is from 0 to m-1, \n",
    "    # command range(m) does this, where m is the number of data points defined above)\n",
    "    \n",
    "    \n",
    "    # return keenel matrix K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data\n",
    "m0 = 50\n",
    "m1 = 100\n",
    "r0 = 1.1*np.sqrt(np.random.uniform(0,1,m0))\n",
    "theta0 = np.random.uniform(0,2*np.pi,m0)\n",
    "r1 = 0.8 + np.sqrt(np.random.uniform(0,1,m1))\n",
    "theta1 = np.random.uniform(0,2*np.pi,m1)\n",
    "X0 = np.column_stack( (r0*np.cos(theta0), r0*np.sin(theta0)) )\n",
    "X1 = np.column_stack( (r1*np.cos(theta1), r1*np.sin(theta1)) )\n",
    "\n",
    "X = np.row_stack((X0,X1))\n",
    "\n",
    "plt.plot(X0[:,0],X0[:,1],'b.',X1[:,0],X1[:,1],'r+',markerfacecolor='none',markersize=8)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gaussian kernel with sigma = 1\n",
    "sigma = 1\n",
    "K_gaus = gaussian_kernel(X,sigma)\n",
    "\n",
    "# Implement kernel support vector machine\n",
    "lamb = 1e-2\n",
    "a_expo,b_expo = kernel_svm(K_gaus,lamb,m0,m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the boundary produced by SVM with the exponential kernel\n",
    "plt.plot(X0[:,0],X0[:,1],'b.',X1[:,0],X1[:,1],'r+',markerfacecolor='none',markersize=8)\n",
    "grid_x = np.linspace(-1.5,1.5,100)\n",
    "grid_y = np.linspace(-1.5,1.5,100)\n",
    "xx,yy = np.meshgrid(grid_x,grid_y)\n",
    "f = sum( ( a_expo[i] * np.exp( -np.sqrt((X[i,0]-xx)**2+(X[i,1]-yy)**2) / (2*sigma**2) ) for i in range(m0+m1) ) )\n",
    "plt.contour(grid_x,grid_y,f-b_expo,0, colors='k',linestyles='dashed')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:  Please use sklearn command to implement kernel support vector machine.\n",
    "Topic of next week?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
